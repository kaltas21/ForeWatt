{
  "consumption": {
    "nhits": {
      "nhits_light": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "nhits_balanced": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "nhits_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "nhits_wide": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "nhits_narrow_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "nhits_price_optimized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "nhits_demand_optimized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "nhits_ultra_light": {
        "error": "list index out of range"
      },
      "nhits_ultra_deep": {
        "error": "Trainer.__init__() got an unexpected keyword argument 'dropout'"
      },
      "nhits_regularized": {
        "error": "Trainer.__init__() got an unexpected keyword argument 'weight_decay'"
      }
    },
    "tft": {
      "tft_light": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_balanced": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_wide": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_narrow_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_price_optimized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_demand_optimized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_ultra_light": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_ultra_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_regularized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      }
    },
    "patchtst": {
      "patchtst_light": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_balanced": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_wide": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_narrow_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_price_optimized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_demand_optimized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_ultra_light": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_ultra_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_regularized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      }
    }
  },
  "price_real": {
    "nhits": {
      "nhits_light": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "nhits_balanced": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "nhits_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "nhits_wide": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "nhits_narrow_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "nhits_price_optimized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "nhits_demand_optimized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "nhits_ultra_light": {
        "error": "list index out of range"
      },
      "nhits_ultra_deep": {
        "error": "Trainer.__init__() got an unexpected keyword argument 'dropout'"
      },
      "nhits_regularized": {
        "error": "Trainer.__init__() got an unexpected keyword argument 'weight_decay'"
      }
    },
    "tft": {
      "tft_light": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_balanced": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_wide": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_narrow_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_price_optimized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_demand_optimized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_ultra_light": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_ultra_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "tft_regularized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      }
    },
    "patchtst": {
      "patchtst_light": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_balanced": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_wide": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_narrow_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_price_optimized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_demand_optimized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_ultra_light": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_ultra_deep": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      },
      "patchtst_regularized": {
        "error": "The operator 'aten::nanmedian.dim_values' is not currently implemented for the MPS device. If you want this op to be considered for addition please comment on https://github.com/pytorch/pytorch/issues/141287 and mention use-case, that resulted in missing op as well as commit hash 4ae33911b227ceda28d6994e5bc7200776a4a6b8. As a temporary fix, you can set the environment variable `PYTORCH_ENABLE_MPS_FALLBACK=1` to use the CPU as a fallback for this op. WARNING: this will be slower than running natively on MPS."
      }
    }
  }
}